{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab663d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SETUP AND INSTALLATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline\n",
    ")\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import gradio as gr\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import PyPDF2\n",
    "import docx\n",
    "from io import StringIO\n",
    "import zipfile\n",
    "import requests\n",
    "from google.colab import files, drive\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: COLAB-OPTIMIZED DATA PROCESSOR\n",
    "# =============================================================================\n",
    "\n",
    "class ColabDataProcessor:\n",
    "    \"\"\"Colab-optimized data processor with file upload support\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processed_data = []\n",
    "        self.raw_text = \"\"\n",
    "\n",
    "    def upload_and_process_files(self):\n",
    "        \"\"\"Upload files through Colab interface\"\"\"\n",
    "        print(\"üìÅ Upload your data structures book (PDF, DOCX, or TXT)\")\n",
    "        print(\"Supported formats: .pdf, .docx, .txt\")\n",
    "\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        all_text = \"\"\n",
    "        for filename, content in uploaded.items():\n",
    "            print(f\"Processing: {filename}\")\n",
    "\n",
    "            if filename.endswith('.pdf'):\n",
    "                text = self.extract_from_pdf(content)\n",
    "            elif filename.endswith('.docx'):\n",
    "                text = self.extract_from_docx(content)\n",
    "            elif filename.endswith('.txt'):\n",
    "                text = content.decode('utf-8')\n",
    "            else:\n",
    "                print(f\"Unsupported file format: {filename}\")\n",
    "                continue\n",
    "\n",
    "            all_text += text + \"\\n\\n\"\n",
    "\n",
    "        self.raw_text = all_text\n",
    "        return all_text\n",
    "\n",
    "    def extract_from_pdf(self, pdf_content):\n",
    "        \"\"\"Extract text from PDF\"\"\"\n",
    "        try:\n",
    "            # Write content to temporary file\n",
    "            with open('temp.pdf', 'wb') as f:\n",
    "                f.write(pdf_content)\n",
    "\n",
    "            # Extract text\n",
    "            text = \"\"\n",
    "            with open('temp.pdf', 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "\n",
    "            # Clean up\n",
    "            os.remove('temp.pdf')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_from_docx(self, docx_content):\n",
    "        \"\"\"Extract text from DOCX\"\"\"\n",
    "        try:\n",
    "            # Write content to temporary file\n",
    "            with open('temp.docx', 'wb') as f:\n",
    "                f.write(docx_content)\n",
    "\n",
    "            # Extract text\n",
    "            doc = docx.Document('temp.docx')\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "\n",
    "            # Clean up\n",
    "            os.remove('temp.docx')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting DOCX: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def use_sample_data(self):\n",
    "        \"\"\"Use sample data structures content for testing\"\"\"\n",
    "        sample_text = \"\"\"\n",
    "        Chapter 1: Arrays\n",
    "        An array is a collection of elements stored in contiguous memory locations. Arrays provide constant-time access to elements using indices.\n",
    "\n",
    "        Key characteristics of arrays:\n",
    "        - Fixed size in most programming languages\n",
    "        - Elements are of the same data type\n",
    "        - Zero-based indexing\n",
    "        - Random access capability\n",
    "\n",
    "        Time complexities:\n",
    "        - Access: O(1)\n",
    "        - Search: O(n) for unsorted, O(log n) for sorted\n",
    "        - Insertion: O(n)\n",
    "        - Deletion: O(n)\n",
    "\n",
    "        Chapter 2: Linked Lists\n",
    "        A linked list is a linear data structure where elements are stored in nodes. Each node contains data and a reference to the next node.\n",
    "\n",
    "        Types of linked lists:\n",
    "        - Singly linked list\n",
    "        - Doubly linked list\n",
    "        - Circular linked list\n",
    "\n",
    "        Advantages:\n",
    "        - Dynamic size\n",
    "        - Efficient insertion and deletion at the beginning\n",
    "        - Memory efficient for sparse data\n",
    "\n",
    "        Time complexities:\n",
    "        - Access: O(n)\n",
    "        - Search: O(n)\n",
    "        - Insertion: O(1) at head, O(n) at arbitrary position\n",
    "        - Deletion: O(1) at head, O(n) at arbitrary position\n",
    "\n",
    "        Chapter 3: Stacks\n",
    "        A stack is a Last-In-First-Out (LIFO) data structure. Elements are added and removed from the same end called the top.\n",
    "\n",
    "        Basic operations:\n",
    "        - Push: Add element to top\n",
    "        - Pop: Remove element from top\n",
    "        - Peek/Top: View top element without removing\n",
    "        - isEmpty: Check if stack is empty\n",
    "\n",
    "        Applications:\n",
    "        - Function call management\n",
    "        - Expression evaluation and syntax parsing\n",
    "        - Undo operations in applications\n",
    "        - Browser history navigation\n",
    "\n",
    "        Chapter 4: Queues\n",
    "        A queue is a First-In-First-Out (FIFO) data structure. Elements are added at the rear and removed from the front.\n",
    "\n",
    "        Basic operations:\n",
    "        - Enqueue: Add element to rear\n",
    "        - Dequeue: Remove element from front\n",
    "        - Front: View front element\n",
    "        - Rear: View rear element\n",
    "\n",
    "        Types:\n",
    "        - Simple queue\n",
    "        - Circular queue\n",
    "        - Priority queue\n",
    "        - Double-ended queue (deque)\n",
    "\n",
    "        Chapter 5: Trees\n",
    "        A tree is a hierarchical data structure consisting of nodes connected by edges. Each tree has a root node and subtrees.\n",
    "\n",
    "        Binary Tree properties:\n",
    "        - Each node has at most two children\n",
    "        - Left and right subtrees are also binary trees\n",
    "        - Height of tree affects performance\n",
    "\n",
    "        Tree traversals:\n",
    "        - Inorder: Left, Root, Right\n",
    "        - Preorder: Root, Left, Right\n",
    "        - Postorder: Left, Right, Root\n",
    "        - Level order: Breadth-first traversal\n",
    "\n",
    "        Chapter 6: Graphs\n",
    "        A graph is a collection of vertices connected by edges. Graphs can represent networks, relationships, and connections.\n",
    "\n",
    "        Types:\n",
    "        - Directed vs Undirected\n",
    "        - Weighted vs Unweighted\n",
    "        - Cyclic vs Acyclic\n",
    "\n",
    "        Representations:\n",
    "        - Adjacency matrix\n",
    "        - Adjacency list\n",
    "\n",
    "        Common algorithms:\n",
    "        - Depth-First Search (DFS)\n",
    "        - Breadth-First Search (BFS)\n",
    "        - Dijkstra's shortest path\n",
    "        - Minimum spanning tree algorithms\n",
    "        \"\"\"\n",
    "\n",
    "        self.raw_text = sample_text\n",
    "        return sample_text\n",
    "\n",
    "    def clean_and_preprocess(self, text):\n",
    "        \"\"\"Clean and preprocess text data\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # Remove special characters but keep important punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:()\\[\\]{}\"\\'`\\-]', '', text)\n",
    "\n",
    "        # Split into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Filter out very short sentences\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def create_training_data(self, sentences):\n",
    "        \"\"\"Create training data from sentences\"\"\"\n",
    "        training_data = []\n",
    "\n",
    "        # Create conversational pairs\n",
    "        for i in range(len(sentences) - 1):\n",
    "            current_sentence = sentences[i]\n",
    "            next_sentence = sentences[i + 1]\n",
    "\n",
    "            # Skip if sentences are too long\n",
    "            if len(current_sentence) > 300 or len(next_sentence) > 300:\n",
    "                continue\n",
    "\n",
    "            # Create question-answer style training data\n",
    "            if any(keyword in current_sentence.lower() for keyword in\n",
    "                   ['array', 'list', 'stack', 'queue', 'tree', 'graph', 'algorithm', 'complexity']):\n",
    "\n",
    "                training_text = f\"Human: Tell me about {current_sentence.lower()}\\nAssistant: {next_sentence}\"\n",
    "                training_data.append(training_text)\n",
    "\n",
    "        # Add direct Q&A pairs\n",
    "        qa_patterns = [\n",
    "            (\"What is\", \"is a\"),\n",
    "            (\"How does\", \"works by\"),\n",
    "            (\"Explain\", \"can be explained as\"),\n",
    "            (\"Define\", \"is defined as\")\n",
    "        ]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for question_start, answer_start in qa_patterns:\n",
    "                if answer_start in sentence.lower():\n",
    "                    # Extract key concept\n",
    "                    words = sentence.split()\n",
    "                    if len(words) > 3:\n",
    "                        concept = \" \".join(words[:3])\n",
    "                        question = f\"{question_start} {concept}?\"\n",
    "                        training_text = f\"Human: {question}\\nAssistant: {sentence}\"\n",
    "                        training_data.append(training_text)\n",
    "                        break\n",
    "\n",
    "        return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 3: GPU-OPTIMIZED MODEL TRAINER\n",
    "# =============================================================================\n",
    "\n",
    "class ColabGPUTrainer:\n",
    "    \"\"\"GPU-optimized trainer for Colab T4\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"microsoft/DialoGPT-small\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = device\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load model optimized for T4 GPU\"\"\"\n",
    "        print(f\"üöÄ Loading {self.model_name} on {self.device}\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "        # Add padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model with optimal settings for T4\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,\n",
    "            device_map=\"auto\" if self.device.type == 'cuda' else None,\n",
    "        )\n",
    "\n",
    "        if self.device.type == 'cuda':\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        return True\n",
    "\n",
    "    def prepare_dataset_gpu(self, training_data):\n",
    "        \"\"\"Prepare dataset optimized for GPU training\"\"\"\n",
    "        print(f\"üìä Preparing {len(training_data)} training examples...\")\n",
    "\n",
    "        # Tokenize all data\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=256,  # Reduced for T4 memory\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        # Convert to dataset\n",
    "        df = pd.DataFrame({'text': training_data})\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "\n",
    "        # Tokenize\n",
    "        def tokenize_batch(examples):\n",
    "            return self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=256\n",
    "            )\n",
    "\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_batch,\n",
    "            batched=True,\n",
    "            remove_columns=['text']\n",
    "        )\n",
    "\n",
    "        # Split train/validation\n",
    "        train_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "        return train_dataset['train'], train_dataset['test']\n",
    "\n",
    "    def train_model_gpu(self, train_dataset, eval_dataset, output_dir=\"./dsbot_model\"):\n",
    "        \"\"\"Train model with T4 GPU optimization\"\"\"\n",
    "\n",
    "        # Training arguments optimized for T4\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=2,  # Reduced for faster training\n",
    "            per_device_train_batch_size=4,  # Optimal for T4\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=2,\n",
    "            warmup_steps=100,\n",
    "            logging_steps=25,\n",
    "            save_steps=250,\n",
    "            eval_steps=250,\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            dataloader_pin_memory=True,\n",
    "            fp16=True,  # Mixed precision for T4\n",
    "            report_to=None,  # Disable wandb\n",
    "            eval_strategy=\"steps\" # Changed from evaluation_strategy to eval_strategy for newer transformers\n",
    "        )\n",
    "\n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "        print(\"üéØ Starting training...\")\n",
    "        print(f\"üìà Training on {len(train_dataset)} examples\")\n",
    "        print(f\"üìä Validating on {len(eval_dataset)} examples\")\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the model\n",
    "        trainer.save_model(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        print(f\"‚úÖ Training completed! Model saved to {output_dir}\")\n",
    "        return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea43743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: COLAB CHATBOT INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "class DSBotColabInterface:\n",
    "    \"\"\"Colab-optimized chatbot interface using Gradio\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.conversation_history = []\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        if self.device.type == 'cuda':\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "    def generate_response(self, user_input, history=None):\n",
    "        \"\"\"Generate response with conversation context\"\"\"\n",
    "        try:\n",
    "            # Format input with context\n",
    "            if history and len(history) > 0:\n",
    "                # Include last few exchanges for context\n",
    "                context = \"\"\n",
    "                for h in history[-3:]:  # Last 3 exchanges\n",
    "                    context += f\"Human: {h[0]}\\nAssistant: {h[1]}\\n\"\n",
    "                formatted_input = f\"{context}Human: {user_input}\\nAssistant:\"\n",
    "            else:\n",
    "                formatted_input = f\"Human: {user_input}\\nAssistant:\"\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer.encode(\n",
    "                formatted_input,\n",
    "                return_tensors='pt',\n",
    "                max_length=400,\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "            if self.device.type == 'cuda':\n",
    "                inputs = inputs.to(self.device)\n",
    "\n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Extract only the new part\n",
    "            if \"Assistant:\" in full_response:\n",
    "                response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "            else:\n",
    "                response = full_response[len(formatted_input):].strip()\n",
    "\n",
    "            # Clean up response\n",
    "            response = response.replace(\"Human:\", \"\").strip()\n",
    "\n",
    "            # Limit response length\n",
    "            if len(response) > 500:\n",
    "                response = response[:500] + \"...\"\n",
    "\n",
    "            return response if response else \"I'm not sure how to respond to that. Could you ask about data structures or algorithms?\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "    def create_gradio_interface(self):\n",
    "        \"\"\"Create Gradio chat interface\"\"\"\n",
    "\n",
    "        def chat_fn(message, history):\n",
    "            \"\"\"Chat function for Gradio\"\"\"\n",
    "            if not message.strip():\n",
    "                return \"\", history\n",
    "\n",
    "            response = self.generate_response(message, history)\n",
    "            history.append([message, response])\n",
    "            return \"\", history\n",
    "\n",
    "        def clear_fn():\n",
    "            \"\"\"Clear chat history\"\"\"\n",
    "            return [], []\n",
    "\n",
    "        def example_fn(example):\n",
    "            \"\"\"Handle example questions\"\"\"\n",
    "            return example, []\n",
    "\n",
    "        # Create interface\n",
    "        with gr.Blocks(\n",
    "            title=\"DSBot - Data Structures Chatbot\",\n",
    "            theme=gr.themes.Soft(),\n",
    "            css=\"\"\"\n",
    "            .gradio-container {\n",
    "                max-width: 800px !important;\n",
    "                margin: auto !important;\n",
    "            }\n",
    "            .chat-message {\n",
    "                padding: 10px !important;\n",
    "                margin: 5px !important;\n",
    "            }\n",
    "            \"\"\"\n",
    "        ) as interface:\n",
    "\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                # ü§ñ DSBot - Data Structures Chatbot\n",
    "\n",
    "                Welcome to DSBot! I'm your AI assistant for data structures and algorithms.\n",
    "                Ask me about arrays, linked lists, trees, graphs, sorting algorithms, and more!\n",
    "\n",
    "                **üöÄ Powered by T4 GPU | üß† Fine-tuned on Data Structures Content**\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=400,\n",
    "                show_label=False,\n",
    "                container=True,\n",
    "                bubble_full_width=False\n",
    "            )\n",
    "\n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    placeholder=\"Ask me about data structures, algorithms, or implementations...\",\n",
    "                    container=False,\n",
    "                    scale=4\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "\n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", scale=1)\n",
    "\n",
    "            # Example questions\n",
    "            gr.Markdown(\"### üí° Try these example questions:\")\n",
    "            example_questions = [\n",
    "                \"What is a binary search tree?\",\n",
    "                \"How does quicksort work?\",\n",
    "                \"Explain hash table collisions\",\n",
    "                \"What's the time complexity of merge sort?\",\n",
    "                \"How do you implement a stack?\",\n",
    "                \"What's the difference between DFS and BFS?\"\n",
    "            ]\n",
    "\n",
    "            with gr.Row():\n",
    "                for i in range(0, len(example_questions), 2):\n",
    "                    with gr.Column():\n",
    "                        for j in range(2):\n",
    "                            if i + j < len(example_questions):\n",
    "                                example_btn = gr.Button(\n",
    "                                    example_questions[i + j],\n",
    "                                    size=\"sm\",\n",
    "                                    variant=\"secondary\"\n",
    "                                )\n",
    "                                example_btn.click(\n",
    "                                    lambda x=example_questions[i + j]: (x, []),\n",
    "                                    outputs=[msg, chatbot]\n",
    "                                )\n",
    "\n",
    "            # Event handlers\n",
    "            msg.submit(chat_fn, [msg, chatbot], [msg, chatbot])\n",
    "            send_btn.click(chat_fn, [msg, chatbot], [msg, chatbot])\n",
    "            clear_btn.click(clear_fn, outputs=[chatbot, msg])\n",
    "\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                ---\n",
    "                **Tips:**\n",
    "                - Ask specific questions about data structures or algorithms\n",
    "                - Request code implementations or examples\n",
    "                - Inquire about time/space complexity analysis\n",
    "                - Get explanations of concepts from your textbook\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        return interface\n",
    "\n",
    "    def launch_interface(self, share=True):\n",
    "        \"\"\"Launch the Gradio interface\"\"\"\n",
    "        interface = self.create_gradio_interface()\n",
    "        print(\"üöÄ Launching DSBot interface...\")\n",
    "        interface.launch(share=share, debug=False, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9178f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: MAIN APPLICATION CONTROLLER\n",
    "# =============================================================================\n",
    "\n",
    "class DSBotColabApp:\n",
    "    \"\"\"Main application controller for Colab\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data_processor = ColabDataProcessor()\n",
    "        self.trainer = ColabGPUTrainer()\n",
    "        self.interface = None\n",
    "\n",
    "    def quick_demo(self):\n",
    "        \"\"\"Quick demo with pre-trained model\"\"\"\n",
    "        print(\"üöÄ Setting up DSBot Demo...\")\n",
    "\n",
    "        # Load pre-trained model\n",
    "        if not self.trainer.load_model():\n",
    "            print(\"‚ùå Failed to load model!\")\n",
    "            return False\n",
    "\n",
    "        # Create interface\n",
    "        self.interface = DSBotColabInterface(\n",
    "            self.trainer.model,\n",
    "            self.trainer.tokenizer\n",
    "        )\n",
    "\n",
    "        # Launch\n",
    "        self.interface.launch_interface()\n",
    "        return True\n",
    "\n",
    "    def full_training_pipeline(self, use_uploaded_data=True):\n",
    "        \"\"\"Complete training pipeline\"\"\"\n",
    "        print(\"üéØ Starting DSBot Full Training Pipeline...\")\n",
    "\n",
    "        # Step 1: Get training data\n",
    "        if use_uploaded_data:\n",
    "            print(\"üìÅ Please upload your book files...\")\n",
    "            raw_text = self.data_processor.upload_and_process_files()\n",
    "        else:\n",
    "            print(\"üìö Using sample data structures content...\")\n",
    "            raw_text = self.data_processor.use_sample_data()\n",
    "\n",
    "        if not raw_text:\n",
    "            print(\"‚ùå No text data available!\")\n",
    "            return False\n",
    "\n",
    "        print(f\"üìä Extracted {len(raw_text)} characters of text\")\n",
    "\n",
    "        # Step 2: Preprocess data\n",
    "        print(\"üîÑ Preprocessing text...\")\n",
    "        sentences = self.data_processor.clean_and_preprocess(raw_text)\n",
    "        training_data = self.data_processor.create_training_data(sentences)\n",
    "\n",
    "        print(f\"‚úÖ Created {len(training_data)} training examples\")\n",
    "\n",
    "        if len(training_data) < 10:\n",
    "            print(\"‚ö†Ô∏è Warning: Very few training examples. Consider using more text data.\")\n",
    "\n",
    "        # Step 3: Load and train model\n",
    "        print(\"ü§ñ Loading base model...\")\n",
    "        if not self.trainer.load_model():\n",
    "            print(\"‚ùå Failed to load base model!\")\n",
    "            return False\n",
    "\n",
    "        print(\"üìà Preparing datasets...\")\n",
    "        train_dataset, eval_dataset = self.trainer.prepare_dataset_gpu(training_data)\n",
    "\n",
    "        print(\"üéØ Training model...\")\n",
    "        model_path = self.trainer.train_model_gpu(train_dataset, eval_dataset)\n",
    "\n",
    "        # Step 4: Create interface with trained model\n",
    "        print(\"üöÄ Setting up trained model interface...\")\n",
    "        self.interface = DSBotColabInterface(\n",
    "            self.trainer.model,\n",
    "            self.trainer.tokenizer\n",
    "        )\n",
    "\n",
    "        # Launch interface\n",
    "        self.interface.launch_interface()\n",
    "\n",
    "        print(\"‚úÖ DSBot training and deployment completed!\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: USAGE EXAMPLES AND EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_dsbot_demo():\n",
    "    \"\"\"Run quick demo\"\"\"\n",
    "    app = DSBotColabApp()\n",
    "    return app.quick_demo()\n",
    "\n",
    "def run_dsbot_training(use_sample_data=False):\n",
    "    \"\"\"Run full training pipeline\"\"\"\n",
    "    app = DSBotColabApp()\n",
    "    return app.full_training_pipeline(use_uploaded_data=not use_sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION INSTRUCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ü§ñ DSBot - Data Structures Chatbot for Google Colab\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Choose your option:\")\n",
    "print(\"1. Quick Demo (uses pre-trained model)\")\n",
    "print(\"2. Full Training (with your book upload)\")\n",
    "print(\"3. Training with Sample Data (for testing)\")\n",
    "print()\n",
    "print(\"üìã Usage:\")\n",
    "print(\"‚Ä¢ For quick demo: run_dsbot_demo()\")\n",
    "print(\"‚Ä¢ For full training: run_dsbot_training()\")\n",
    "print(\"‚Ä¢ For sample data training: run_dsbot_training(use_sample_data=True)\")\n",
    "print()\n",
    "print(\"üí° Tips for T4 GPU:\")\n",
    "print(\"‚Ä¢ Training typically takes 15-30 minutes\")\n",
    "print(\"‚Ä¢ Use batch size 4 for optimal memory usage\")\n",
    "print(\"‚Ä¢ Mixed precision (fp16) is enabled for speed\")\n",
    "print(\"‚Ä¢ Model will be saved in ./dsbot_model/\")\n",
    "print()\n",
    "print(\"üöÄ Ready to start! Run one of the functions above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
